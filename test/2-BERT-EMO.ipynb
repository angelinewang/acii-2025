{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "/Users/angwang/miniforge3/lib/python3.12/site-packages/transformers/pipelines/text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the pre-trained model and tokenizer\n",
    "model_name = \"bhadresh-savani/bert-base-uncased-emotion\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Create the emotion classifier pipeline\n",
    "classifier = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_all_scores=True,\n",
    "    device=0 if torch.cuda.is_available() else -1  # Use GPU if available\n",
    ")\n",
    "\n",
    "def get_predictions_batch(texts, threshold=0.5, batch_size=16):\n",
    "    \"\"\"Get predictions ensuring 5 labels\"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Predicting\"):\n",
    "        batch_texts = [\" \".join(tokens) if isinstance(tokens, list) else str(tokens) \n",
    "                      for tokens in texts[i:i + batch_size]]\n",
    "        \n",
    "        inputs = tokenizer(\n",
    "            batch_texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            probs = torch.sigmoid(outputs.logits)\n",
    "            batch_preds = (probs > threshold).int().cpu().numpy()\n",
    "            \n",
    "            # Ensure 5 labels\n",
    "            if batch_preds.shape[1] != 5:\n",
    "                batch_preds = batch_preds[:, :5]\n",
    "            \n",
    "            predictions.extend(batch_preds)\n",
    "    \n",
    "    return np.array(predictions)\n",
    "\n",
    "def format_predictions(predictions, emotion_mapping):\n",
    "    \"\"\"\n",
    "    Convert raw predictions to multi-hot format\n",
    "    Each text can have multiple emotions (multi-label classification)\n",
    "    \"\"\"\n",
    "    formatted_preds = []\n",
    "    for pred in predictions:\n",
    "        # Initialize zeros for all emotions\n",
    "        emotion_scores = {e: 0 for e in emotions}\n",
    "        \n",
    "        # Update scores for predicted emotions\n",
    "        for p in pred:\n",
    "            mapped_emotion = emotion_mapping.get(p['label'])\n",
    "            if mapped_emotion:\n",
    "                emotion_scores[mapped_emotion] = p['score']\n",
    "        \n",
    "        # Convert to list in the correct order\n",
    "        row = [emotion_scores[e] for e in emotions]\n",
    "        formatted_preds.append(row)\n",
    "    \n",
    "    return np.array(formatted_preds)\n",
    "\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv('../public_data_test/track_a/train/eng.csv')\n",
    "val = pd.read_csv('../public_data_test/track_a/dev/eng.csv')\n",
    "test = pd.read_csv('../public_data_test/track_a/test/eng.csv')\n",
    "\n",
    "# Extract text from datasets\n",
    "val_text = val['text'].tolist()\n",
    "test_text = test['text'].tolist()\n",
    "\n",
    "# Define emotions and mapping (model's labels to our labels)\n",
    "emotions = ['joy', 'sadness', 'surprise', 'fear', 'anger']\n",
    "emotion_mapping = {\n",
    "    'joy': 'joy',\n",
    "    'sadness': 'sadness',\n",
    "    'surprise': 'surprise',\n",
    "    'fear': 'fear',\n",
    "    'anger': 'anger',\n",
    "    'love': None  # We'll ignore this emotion as it's not in our target set\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Preprocessing Config\n",
    "# # config = {\n",
    "# #     'sep_pn': True,      # Separate punctuation\n",
    "# #     'rm_pn': False,      # Remove punctuation\n",
    "# #     'apply_lemmatization': True,\n",
    "# #     'apply_stemming': True,\n",
    "# #     'add_bigrams': True,\n",
    "# #     'rm_sw': False       # Remove stopwords\n",
    "# # }\n",
    "\n",
    "# def pre_process(text, config):\n",
    "#     \"\"\"Preprocess text with multiple options\"\"\"\n",
    "#     def separate_punctuation(text):\n",
    "#         # Fixed quotation marks in regex patterns\n",
    "#         text = re.sub(r\"(\\w)([.,;:!?'\\\"\\)])\", r\"\\1 \\2\", text)\n",
    "#         text = re.sub(r\"([.,;:!?'\\\"\\(\\)])(\\w)\", r\"\\1 \\2\", text)\n",
    "#         return text\n",
    "\n",
    "#     def remove_punctuation(text):\n",
    "#         # Fixed quotation marks in regex pattern\n",
    "#         text = re.sub(r\"[.,;:!?'\\\"\\(\\)]\", \"\", text)\n",
    "#         return text\n",
    "\n",
    "#     # Apply preprocessing steps based on config\n",
    "#     if config['sep_pn'] and not config['rm_pn']:\n",
    "#         text = separate_punctuation(text)\n",
    "#     if config['rm_pn'] and not config['sep_pn']:\n",
    "#         text = remove_punctuation(text)\n",
    "\n",
    "#     # Tokenize\n",
    "#     doc = nlp(text.lower())\n",
    "#     tokens = [token.text for token in doc]\n",
    "\n",
    "#     # Apply stemming and lemmatization\n",
    "#     if config['apply_stemming']:\n",
    "#         tokens = [stemmer.stem(token) for token in tokens]\n",
    "#     if config['apply_lemmatization']:\n",
    "#         tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "#     # Generate bigrams if configured\n",
    "#     if config['add_bigrams']:\n",
    "#         bigrams = [\" \".join(gram) for gram in ngrams(tokens, 2)]\n",
    "#         tokens.extend(bigrams)\n",
    "\n",
    "#     # Remove stopwords if configured\n",
    "#     if config['rm_sw']:\n",
    "#         stop_words = set(stopwords.words('english'))\n",
    "#         tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "#     return \" \".join(tokens)\n",
    "\n",
    "# print(\"Preprocessing texts...\")\n",
    "# val_text = [pre_process(text, config) for text in tqdm(val['text'], desc=\"Preprocessing validation\")]\n",
    "# test_text = [pre_process(text, config) for text in tqdm(test['text'], desc=\"Preprocessing test\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import jaccard_score, recall_score, precision_score, f1_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "import spacy\n",
    "import re\n",
    "import datetime\n",
    "\n",
    "# Initialize NLP tools\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /Users/angwang/miniforge3/lib/python3.12/site-packages/huggingface_hub-0.27.1-py3.8.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: emoji in /Users/angwang/miniforge3/lib/python3.12/site-packages (2.14.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Using device: cpu\n",
      "Testing preprocessing function...\n",
      "Token length: 138\n",
      "Within limit: True\n"
     ]
    }
   ],
   "source": [
    "!pip install emoji\n",
    "import emoji\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Basic text cleaning\"\"\"\n",
    "    # Convert to lowercase\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    # Remove phone numbers\n",
    "    text = re.sub(r'\\+?[\\d\\-\\(\\) ]{8,}', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "def handle_emojis(text):\n",
    "    \"\"\"Convert emojis to text descriptions\"\"\"\n",
    "    return emoji.demojize(text, delimiters=(\" \", \" \"))\n",
    "\n",
    "def expand_contractions(text):\n",
    "    \"\"\"Expand common contractions\"\"\"\n",
    "    contractions = {\n",
    "        \"ain't\": \"am not\",\n",
    "        \"aren't\": \"are not\",\n",
    "        \"can't\": \"cannot\",\n",
    "        \"couldn't\": \"could not\",\n",
    "        \"didn't\": \"did not\",\n",
    "        \"doesn't\": \"does not\",\n",
    "        \"don't\": \"do not\",\n",
    "        \"hadn't\": \"had not\",\n",
    "        \"hasn't\": \"has not\",\n",
    "        \"haven't\": \"have not\",\n",
    "        \"he'd\": \"he would\",\n",
    "        \"he'll\": \"he will\",\n",
    "        \"he's\": \"he is\",\n",
    "        \"i'd\": \"i would\",\n",
    "        \"i'll\": \"i will\",\n",
    "        \"i'm\": \"i am\",\n",
    "        \"i've\": \"i have\",\n",
    "        \"isn't\": \"is not\",\n",
    "        \"it's\": \"it is\",\n",
    "        \"let's\": \"let us\",\n",
    "        \"shouldn't\": \"should not\",\n",
    "        \"that's\": \"that is\",\n",
    "        \"there's\": \"there is\",\n",
    "        \"they'd\": \"they would\",\n",
    "        \"they'll\": \"they will\",\n",
    "        \"they're\": \"they are\",\n",
    "        \"they've\": \"they have\",\n",
    "        \"wasn't\": \"was not\",\n",
    "        \"we'd\": \"we would\",\n",
    "        \"we're\": \"we are\",\n",
    "        \"weren't\": \"were not\",\n",
    "        \"what's\": \"what is\",\n",
    "        \"where's\": \"where is\",\n",
    "        \"who's\": \"who is\",\n",
    "        \"won't\": \"will not\",\n",
    "        \"wouldn't\": \"would not\",\n",
    "        \"you'd\": \"you would\",\n",
    "        \"you'll\": \"you will\",\n",
    "        \"you're\": \"you are\",\n",
    "        \"you've\": \"you have\"\n",
    "    }\n",
    "    for contraction, expansion in contractions.items():\n",
    "        text = text.replace(contraction, expansion)\n",
    "    return text\n",
    "\n",
    "def normalize_elongated_words(text):\n",
    "    \"\"\"Normalize words with repeated characters\"\"\"\n",
    "    return re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n",
    "\n",
    "def pre_process(text, config):\n",
    "    \"\"\"Preprocessing function with careful string handling\"\"\"\n",
    "    MAX_LENGTH = 510  # 512 - 2 for [CLS] and [SEP]\n",
    "    \n",
    "    try:\n",
    "        # Ensure text is a string and handle initial cleaning\n",
    "        text = str(text)\n",
    "        text = clean_text(text)\n",
    "        \n",
    "        # Basic preprocessing\n",
    "        if config.get('handle_emojis', False):\n",
    "            text = handle_emojis(text)\n",
    "        if config.get('expand_contractions', False):\n",
    "            text = expand_contractions(text)\n",
    "        if config.get('normalize_elongated', False):\n",
    "            text = normalize_elongated_words(text)\n",
    "        \n",
    "        # Process with spaCy\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        # Get base tokens with stopword removal if configured\n",
    "        if config.get('rm_sw', False):\n",
    "            stop_words = set(stopwords.words('english'))\n",
    "            # Handle stopwords with explicit string conversion\n",
    "            base_tokens = [str(token.text) for token in doc \n",
    "                         if str(token.text).lower() not in stop_words]\n",
    "        else:\n",
    "            base_tokens = [str(token.text) for token in doc]\n",
    "        \n",
    "        # Check length and truncate if needed\n",
    "        base_text = \" \".join(base_tokens)\n",
    "        if len(tokenizer.tokenize(base_text)) > MAX_LENGTH:\n",
    "            return tokenizer.tokenize(base_text)[:MAX_LENGTH]\n",
    "        \n",
    "        # Apply lemmatization/stemming if configured\n",
    "        if config.get('apply_lemmatization', False):\n",
    "            base_tokens = [str(token.lemma_) for token in doc]\n",
    "        elif config.get('apply_stemming', False):\n",
    "            stemmer = PorterStemmer()\n",
    "            base_tokens = [stemmer.stem(str(token)) for token in base_tokens]\n",
    "        \n",
    "        # Initialize features\n",
    "        features = []\n",
    "        current_length = len(tokenizer.tokenize(\" \".join(base_tokens)))\n",
    "        remaining_space = MAX_LENGTH - current_length\n",
    "        \n",
    "        # Add features if space permits\n",
    "        if remaining_space > 0:\n",
    "            # Add trigrams\n",
    "            if config.get('add_trigrams', False) and len(base_tokens) >= 3:\n",
    "                for i in range(len(base_tokens) - 2):\n",
    "                    trigram = \" \".join(base_tokens[i:i+3])\n",
    "                    if len(tokenizer.tokenize(trigram)) <= remaining_space:\n",
    "                        features.append(trigram)\n",
    "                        remaining_space -= len(tokenizer.tokenize(trigram))\n",
    "                    else:\n",
    "                        break\n",
    "            \n",
    "            # Add POS tags\n",
    "            if config.get('add_pos_tags', False):\n",
    "                for token in doc:\n",
    "                    pos_tag = f\"{str(token.text)}_{token.pos_}\"\n",
    "                    if len(tokenizer.tokenize(pos_tag)) <= remaining_space:\n",
    "                        features.append(pos_tag)\n",
    "                        remaining_space -= len(tokenizer.tokenize(pos_tag))\n",
    "                    else:\n",
    "                        break\n",
    "            \n",
    "            # Add dependency tags\n",
    "            if config.get('add_dep_tags', False):\n",
    "                for token in doc:\n",
    "                    dep_tag = f\"{str(token.text)}_{token.dep_}\"\n",
    "                    if len(tokenizer.tokenize(dep_tag)) <= remaining_space:\n",
    "                        features.append(dep_tag)\n",
    "                        remaining_space -= len(tokenizer.tokenize(dep_tag))\n",
    "                    else:\n",
    "                        break\n",
    "        \n",
    "        # Combine and verify final length\n",
    "        all_tokens = base_tokens + features\n",
    "        final_text = \" \".join(all_tokens)\n",
    "        \n",
    "        if len(tokenizer.tokenize(final_text)) > MAX_LENGTH:\n",
    "            return tokenizer.tokenize(final_text)[:MAX_LENGTH]\n",
    "        \n",
    "        return all_tokens\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in preprocessing: {str(e)}\")\n",
    "        # Fallback to basic tokenization\n",
    "        return tokenizer.tokenize(str(text))[:MAX_LENGTH]\n",
    "\n",
    "# Test the function\n",
    "test_config = {\n",
    "    'sep_pn': False,\n",
    "    'rm_pn': False,\n",
    "    'apply_lemmatization': False,\n",
    "    'apply_stemming': False,\n",
    "    'add_bigrams': False,\n",
    "    'add_trigrams': True,\n",
    "    'rm_sw': True,\n",
    "    'handle_emojis': True,\n",
    "    'expand_contractions': True,\n",
    "    'normalize_elongated': True,\n",
    "    'add_pos_tags': True,\n",
    "    'add_dep_tags': True,\n",
    "    'use_tfidf': True,\n",
    "    'tfidf_max_features': 1000,\n",
    "    'tfidf_ngram_range': 1,\n",
    "    'tfidf_min_df': 2,\n",
    "    'tfidf_max_df': 0.95\n",
    "}\n",
    "\n",
    "# Verify function works\n",
    "print(\"Testing preprocessing function...\")\n",
    "sample_text = val['text'].iloc[0]\n",
    "processed_tokens = pre_process(sample_text, test_config)\n",
    "token_length = len(tokenizer.tokenize(\" \".join(processed_tokens)))\n",
    "print(f\"Token length: {token_length}\")\n",
    "print(f\"Within limit: {token_length <= 510}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting validation predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:   0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Predicting: 100%|██████████| 8/8 [00:03<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting test predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 173/173 [01:32<00:00,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions to ../results/val_predictions_2025-01-30_12_27_50.csv\n",
      "Saved predictions to ../results/test_predictions_2025-01-30_12_27_50.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Get predictions\n",
    "print(\"Getting validation predictions...\")\n",
    "val_predictions = get_predictions_batch(val_text, threshold=0.5)\n",
    "\n",
    "print(\"Getting test predictions...\")\n",
    "test_predictions = get_predictions_batch(test_text, threshold=0.5)\n",
    "\n",
    "# Convert probabilities to binary predictions (you can adjust the threshold)\n",
    "threshold = 0.5\n",
    "val_binary_preds = (val_predictions > threshold).astype(int)\n",
    "test_binary_preds = (test_predictions > threshold).astype(int)\n",
    "\n",
    "# Save predictions\n",
    "def save_predictions(predictions, ids, filename):\n",
    "    \"\"\"Save multi-hot predictions to CSV\"\"\"\n",
    "    df_predictions = pd.DataFrame(predictions, columns=emotions)\n",
    "    df_predictions['id'] = ids\n",
    "    df_predictions = df_predictions[['id'] + emotions]\n",
    "    df_predictions.to_csv(filename, index=False)\n",
    "    print(f\"Saved predictions to {filename}\")\n",
    "\n",
    "# Save validation and test predictions\n",
    "from datetime import datetime\n",
    "# Save validation and test predictions\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d_%H_%M_%S')\n",
    "\n",
    "save_predictions(\n",
    "    val_predictions,\n",
    "    val['id'],\n",
    "    f'../results/val_predictions_{timestamp}.csv'\n",
    ")\n",
    "save_predictions(\n",
    "    test_predictions,\n",
    "    test['id'],\n",
    "    f'../results/test_predictions_{timestamp}.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import jaccard_score, recall_score, precision_score, f1_score\n",
    "\n",
    "def evaluate(y_true, y_pred):\n",
    "    # Calculate Jaccard score\n",
    "    jaccard = jaccard_score(y_true, y_pred, average='samples')\n",
    "    print(f'Multilabel accuracy (Jaccard score): {round(jaccard, 4)}')\n",
    "    \n",
    "    \"\"\"Evaluate with micro and macro metrics for multi-label classification\"\"\"\n",
    "    for average in ['micro', 'macro']:\n",
    "        recall = recall_score(y_true, y_pred, average=average, zero_division=0)\n",
    "        precision = precision_score(y_true, y_pred, average=average, zero_division=0)\n",
    "        f1 = f1_score(y_true, y_pred, average=average, zero_division=0)\n",
    "    \n",
    "        print(f'{average.upper()} recall: {round(recall, 4)}, '\n",
    "              f'precision: {round(precision, 4)}, '\n",
    "              f'f1: {round(f1, 4)}')\n",
    "\n",
    "def evaluate_per_class(y_true, y_pred):\n",
    "    \"\"\"Evaluate metrics for each emotion separately\"\"\"\n",
    "    for i, emotion in enumerate(emotions):\n",
    "        print(f'*** {emotion} ***')\n",
    "    \n",
    "        recall = recall_score(y_true[:,i], y_pred[:,i], zero_division=0)\n",
    "        precision = precision_score(y_true[:,i], y_pred[:,i], zero_division=0)\n",
    "        f1 = f1_score(y_true[:,i], y_pred[:,i], zero_division=0)\n",
    "        \n",
    "        print(f'recall: {round(recall, 4)}, '\n",
    "              f'precision: {round(precision, 4)}, '\n",
    "              f'f1: {round(f1, 4)}\\n')\n",
    "\n",
    "# After getting predictions, add this evaluation code:\n",
    "# Evaluate predictions\n",
    "# print(\"\\nEvaluating validation predictions...\")\n",
    "# val_true = val[emotions].values\n",
    "# print(\"Overall Metrics:\")\n",
    "# evaluate(val_true, val_predictions)\n",
    "# print(\"\\nPer-class Metrics:\")\n",
    "# evaluate_per_class(val_true, val_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "def create_tfidf_features(train_texts, val_texts, test_texts, config):\n",
    "    \"\"\"Create TF-IDF features for texts\"\"\"\n",
    "    # Initialize TF-IDF vectorizer with parameters from config\n",
    "    tfidf = TfidfVectorizer(\n",
    "        max_features=config.get('tfidf_max_features', 1000),\n",
    "        ngram_range=(1, config.get('tfidf_ngram_range', 1)),\n",
    "        min_df=config.get('tfidf_min_df', 2),\n",
    "        max_df=config.get('tfidf_max_df', 0.95)\n",
    "    )\n",
    "    \n",
    "    # Fit on training data\n",
    "    tfidf.fit(train_texts)\n",
    "    \n",
    "    # Transform all datasets\n",
    "    train_tfidf = tfidf.transform(train_texts)\n",
    "    val_tfidf = tfidf.transform(val_texts)\n",
    "    test_tfidf = tfidf.transform(test_texts)\n",
    "    \n",
    "    return train_tfidf, val_tfidf, test_tfidf, tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        \"\"\"Initialize dataset with texts and labels\"\"\"\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Get a single item\"\"\"\n",
    "        text = str(self.texts[idx])  # Ensure text is string\n",
    "        label = self.labels[idx]     # Get corresponding label\n",
    "        \n",
    "        # Tokenize text\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Remove batch dimension and create item dictionary\n",
    "        item = {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(label, dtype=torch.float)\n",
    "        }\n",
    "        \n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Get length of dataset\"\"\"\n",
    "        return len(self.texts)\n",
    "\n",
    "def collate_batch(batch):\n",
    "    \"\"\"Custom collate function to ensure proper batching\"\"\"\n",
    "    input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "    labels = torch.stack([item['labels'] for item in batch])\n",
    "    \n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "from transformers import AutoConfig\n",
    "\n",
    "def fine_tune_model(model, train_texts, train_labels, val_texts, val_labels, config):\n",
    "    \"\"\"Fine-tune BERT model with properly initialized weights\"\"\"\n",
    "    BATCH_SIZE = 16\n",
    "    \n",
    "    # Update model config for our 5 emotions\n",
    "    model_config = AutoConfig.from_pretrained(model_name)\n",
    "    model_config.problem_type = \"multi_label_classification\"\n",
    "    model_config.num_labels = 5  # Our 5 emotions\n",
    "    \n",
    "    # Load pre-trained model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        config=model_config,\n",
    "        ignore_mismatched_sizes=True\n",
    "    )\n",
    "    \n",
    "    # Initialize the new classifier weights properly\n",
    "    torch.nn.init.xavier_uniform_(model.classifier.weight)\n",
    "    torch.nn.init.zeros_(model.classifier.bias)\n",
    "    \n",
    "    print(\"\\nModel configuration:\")\n",
    "    print(f\"Classifier weight shape: {model.classifier.weight.shape}\")\n",
    "    print(f\"Classifier bias shape: {model.classifier.bias.shape}\")\n",
    "    print(f\"Number of labels: {model_config.num_labels}\")\n",
    "    print(f\"Emotion labels: {emotions}\")\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = EmotionDataset(train_texts, train_labels, tokenizer)\n",
    "    val_dataset = EmotionDataset(val_texts, val_labels, tokenizer)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_batch,\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_batch,\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config.get('learning_rate', 2e-5),\n",
    "        eps=config.get('adam_epsilon', 1e-8)\n",
    "    )\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    # Training setup\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(config.get('num_epochs', 3)):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        \n",
    "        for batch_idx, batch in enumerate(tqdm(train_loader, desc=f'Epoch {epoch + 1}')):\n",
    "            # Move batch to device\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            labels = batch.pop('labels')\n",
    "            \n",
    "            # Clear gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            try:\n",
    "                # Forward pass\n",
    "                outputs = model(**batch)\n",
    "                logits = outputs.logits\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = criterion(logits, labels)\n",
    "                \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                \n",
    "                # Update weights\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_train_loss += loss.item()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in batch {batch_idx}: {str(e)}\")\n",
    "                print(f\"Shapes - Input IDs: {batch['input_ids'].shape}, \"\n",
    "                      f\"Logits: {logits.shape if 'logits' in locals() else 'N/A'}, \"\n",
    "                      f\"Labels: {labels.shape}\")\n",
    "                continue\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                labels = batch.pop('labels')\n",
    "                \n",
    "                try:\n",
    "                    outputs = model(**batch)\n",
    "                    logits = outputs.logits\n",
    "                    loss = criterion(logits, labels)\n",
    "                    total_val_loss += loss.item()\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in validation: {str(e)}\")\n",
    "                    continue\n",
    "        \n",
    "        # Print epoch results\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        \n",
    "        print(f'\\nEpoch {epoch + 1}:')\n",
    "        print(f'Average training loss: {avg_train_loss:.4f}')\n",
    "        print(f'Average validation loss: {avg_val_loss:.4f}')\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "    \n",
    "    # Restore best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Predictions \n",
    "def save_predictions(predictions, ids, filename):\n",
    "    \"\"\"Save multi-hot predictions to CSV\"\"\"\n",
    "    # Create DataFrame with predictions\n",
    "    df_predictions = pd.DataFrame(predictions, columns=emotions)\n",
    "    \n",
    "    # Add ID column\n",
    "    df_predictions['id'] = ids\n",
    "    \n",
    "    # Reorder columns to put ID first\n",
    "    df_predictions = df_predictions[['id'] + emotions]\n",
    "    \n",
    "    # Save to CSV\n",
    "    df_predictions.to_csv(filename, index=False)\n",
    "    print(f\"Saved predictions to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /Users/angwang/miniforge3/lib/python3.12/site-packages/huggingface_hub-0.27.1-py3.8.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: emoji in /Users/angwang/miniforge3/lib/python3.12/site-packages (2.14.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Starting preprocessing grid search...\n",
      "Testing 4 configurations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 8/8 [00:33<00:00,  4.15s/it]\n",
      "Grid Search:  25%|██▌       | 1/4 [01:19<03:57, 79.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best Macro F1: 0.2704\n",
      "Configuration:\n",
      "  sep_pn: False\n",
      "  rm_pn: False\n",
      "  apply_lemmatization: False\n",
      "  apply_stemming: False\n",
      "  add_bigrams: False\n",
      "  add_trigrams: True\n",
      "  rm_sw: False\n",
      "  handle_emojis: True\n",
      "  expand_contractions: True\n",
      "  normalize_elongated: True\n",
      "  add_pos_tags: True\n",
      "  add_dep_tags: True\n",
      "  use_tfidf: True\n",
      "  tfidf_max_features: 200\n",
      "  tfidf_ngram_range: 1\n",
      "  tfidf_min_df: 2\n",
      "  tfidf_max_df: 0.9\n",
      "\n",
      "Detailed Metrics:\n",
      "Multilabel accuracy (Jaccard score): 0.2533\n",
      "MICRO recall: 0.5057, precision: 0.3134, f1: 0.387\n",
      "MACRO recall: 0.3964, precision: 0.2085, f1: 0.2704\n",
      "*** joy ***\n",
      "recall: 0.3871, precision: 0.1739, f1: 0.24\n",
      "\n",
      "*** sadness ***\n",
      "recall: 0.6286, precision: 0.25, f1: 0.3577\n",
      "\n",
      "*** surprise ***\n",
      "recall: 0.0, precision: 0.0, f1: 0.0\n",
      "\n",
      "*** fear ***\n",
      "recall: 0.8413, precision: 0.5354, f1: 0.6543\n",
      "\n",
      "*** anger ***\n",
      "recall: 0.125, precision: 0.0833, f1: 0.1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 8/8 [00:39<00:00,  4.93s/it]\n",
      "Predicting: 100%|██████████| 8/8 [00:31<00:00,  3.96s/it]]\n",
      "Grid Search:  75%|███████▌  | 3/4 [03:53<01:16, 76.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best Macro F1: 0.2714\n",
      "Configuration:\n",
      "  sep_pn: False\n",
      "  rm_pn: False\n",
      "  apply_lemmatization: False\n",
      "  apply_stemming: False\n",
      "  add_bigrams: False\n",
      "  add_trigrams: False\n",
      "  rm_sw: False\n",
      "  handle_emojis: True\n",
      "  expand_contractions: True\n",
      "  normalize_elongated: True\n",
      "  add_pos_tags: True\n",
      "  add_dep_tags: True\n",
      "  use_tfidf: True\n",
      "  tfidf_max_features: 200\n",
      "  tfidf_ngram_range: 1\n",
      "  tfidf_min_df: 2\n",
      "  tfidf_max_df: 0.9\n",
      "\n",
      "Detailed Metrics:\n",
      "Multilabel accuracy (Jaccard score): 0.2466\n",
      "MICRO recall: 0.5057, precision: 0.3145, f1: 0.3878\n",
      "MACRO recall: 0.3961, precision: 0.2082, f1: 0.2714\n",
      "*** joy ***\n",
      "recall: 0.4839, precision: 0.2206, f1: 0.303\n",
      "\n",
      "*** sadness ***\n",
      "recall: 0.5143, precision: 0.2222, f1: 0.3103\n",
      "\n",
      "*** surprise ***\n",
      "recall: 0.0, precision: 0.0, f1: 0.0\n",
      "\n",
      "*** fear ***\n",
      "recall: 0.8571, precision: 0.5243, f1: 0.6506\n",
      "\n",
      "*** anger ***\n",
      "recall: 0.125, precision: 0.0741, f1: 0.093\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 8/8 [00:38<00:00,  4.85s/it]\n",
      "Grid Search: 100%|██████████| 4/4 [05:10<00:00, 77.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Best Configuration (Macro F1):\n",
      "Best Macro F1: 0.2714\n",
      "Configuration:\n",
      "  sep_pn: False\n",
      "  rm_pn: False\n",
      "  apply_lemmatization: False\n",
      "  apply_stemming: False\n",
      "  add_bigrams: False\n",
      "  add_trigrams: False\n",
      "  rm_sw: False\n",
      "  handle_emojis: True\n",
      "  expand_contractions: True\n",
      "  normalize_elongated: True\n",
      "  add_pos_tags: True\n",
      "  add_dep_tags: True\n",
      "  use_tfidf: True\n",
      "  tfidf_max_features: 200\n",
      "  tfidf_ngram_range: 1\n",
      "  tfidf_min_df: 2\n",
      "  tfidf_max_df: 0.9\n",
      "\n",
      "Best Configurations Found:\n",
      "Best Jaccard Score: 0.2533\n",
      "Configuration:\n",
      "  sep_pn: False\n",
      "  rm_pn: False\n",
      "  apply_lemmatization: False\n",
      "  apply_stemming: False\n",
      "  add_bigrams: False\n",
      "  add_trigrams: True\n",
      "  rm_sw: False\n",
      "  handle_emojis: True\n",
      "  expand_contractions: True\n",
      "  normalize_elongated: True\n",
      "  add_pos_tags: True\n",
      "  add_dep_tags: True\n",
      "  use_tfidf: True\n",
      "  tfidf_max_features: 200\n",
      "  tfidf_ngram_range: 1\n",
      "  tfidf_min_df: 2\n",
      "  tfidf_max_df: 0.9\n",
      "\n",
      "Best Micro F1: 0.3878\n",
      "Configuration:\n",
      "  sep_pn: False\n",
      "  rm_pn: False\n",
      "  apply_lemmatization: False\n",
      "  apply_stemming: False\n",
      "  add_bigrams: False\n",
      "  add_trigrams: False\n",
      "  rm_sw: False\n",
      "  handle_emojis: True\n",
      "  expand_contractions: True\n",
      "  normalize_elongated: True\n",
      "  add_pos_tags: True\n",
      "  add_dep_tags: True\n",
      "  use_tfidf: True\n",
      "  tfidf_max_features: 200\n",
      "  tfidf_ngram_range: 1\n",
      "  tfidf_min_df: 2\n",
      "  tfidf_max_df: 0.9\n",
      "\n",
      "Best Macro F1: 0.2714\n",
      "Configuration:\n",
      "  sep_pn: False\n",
      "  rm_pn: False\n",
      "  apply_lemmatization: False\n",
      "  apply_stemming: False\n",
      "  add_bigrams: False\n",
      "  add_trigrams: False\n",
      "  rm_sw: False\n",
      "  handle_emojis: True\n",
      "  expand_contractions: True\n",
      "  normalize_elongated: True\n",
      "  add_pos_tags: True\n",
      "  add_dep_tags: True\n",
      "  use_tfidf: True\n",
      "  tfidf_max_features: 200\n",
      "  tfidf_ngram_range: 1\n",
      "  tfidf_min_df: 2\n",
      "  tfidf_max_df: 0.9\n",
      "\n",
      "Best Per-Emotion F1 Scores:\n",
      "joy: 0.3030\n",
      "Config: {'sep_pn': False, 'rm_pn': False, 'apply_lemmatization': False, 'apply_stemming': False, 'add_bigrams': False, 'add_trigrams': False, 'rm_sw': False, 'handle_emojis': True, 'expand_contractions': True, 'normalize_elongated': True, 'add_pos_tags': True, 'add_dep_tags': True, 'use_tfidf': True, 'tfidf_max_features': 200, 'tfidf_ngram_range': 1, 'tfidf_min_df': 2, 'tfidf_max_df': 0.9}\n",
      "sadness: 0.3577\n",
      "Config: {'sep_pn': False, 'rm_pn': False, 'apply_lemmatization': False, 'apply_stemming': False, 'add_bigrams': False, 'add_trigrams': True, 'rm_sw': False, 'handle_emojis': True, 'expand_contractions': True, 'normalize_elongated': True, 'add_pos_tags': True, 'add_dep_tags': True, 'use_tfidf': True, 'tfidf_max_features': 200, 'tfidf_ngram_range': 1, 'tfidf_min_df': 2, 'tfidf_max_df': 0.9}\n",
      "surprise: 0.0000\n",
      "Config: None\n",
      "fear: 0.6543\n",
      "Config: {'sep_pn': False, 'rm_pn': False, 'apply_lemmatization': False, 'apply_stemming': False, 'add_bigrams': False, 'add_trigrams': True, 'rm_sw': False, 'handle_emojis': True, 'expand_contractions': True, 'normalize_elongated': True, 'add_pos_tags': True, 'add_dep_tags': True, 'use_tfidf': True, 'tfidf_max_features': 200, 'tfidf_ngram_range': 1, 'tfidf_min_df': 2, 'tfidf_max_df': 0.9}\n",
      "anger: 0.1000\n",
      "Config: {'sep_pn': False, 'rm_pn': False, 'apply_lemmatization': False, 'apply_stemming': False, 'add_bigrams': False, 'add_trigrams': True, 'rm_sw': False, 'handle_emojis': True, 'expand_contractions': True, 'normalize_elongated': True, 'add_pos_tags': True, 'add_dep_tags': True, 'use_tfidf': True, 'tfidf_max_features': 200, 'tfidf_ngram_range': 1, 'tfidf_min_df': 2, 'tfidf_max_df': 0.9}\n",
      "\n",
      "Detailed results saved to: ../results/preprocessing_grid_search_2025-01-30_12_33_01.csv\n",
      "\n",
      "Generating final predictions with best Macro F1 configuration...\n",
      "Preprocessing texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing validation: 100%|██████████| 116/116 [00:00<00:00, 123.67it/s]\n",
      "Preprocessing test: 100%|██████████| 2767/2767 [00:19<00:00, 142.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting validation predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 8/8 [00:40<00:00,  5.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting test predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 173/173 [18:06<00:00,  6.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Evaluation with Best Macro F1 Configuration:\n",
      "Overall Metrics:\n",
      "Multilabel accuracy (Jaccard score): 0.2466\n",
      "MICRO recall: 0.5057, precision: 0.3145, f1: 0.3878\n",
      "MACRO recall: 0.3961, precision: 0.2082, f1: 0.2714\n",
      "\n",
      "Per-class Metrics:\n",
      "*** joy ***\n",
      "recall: 0.4839, precision: 0.2206, f1: 0.303\n",
      "\n",
      "*** sadness ***\n",
      "recall: 0.5143, precision: 0.2222, f1: 0.3103\n",
      "\n",
      "*** surprise ***\n",
      "recall: 0.0, precision: 0.0, f1: 0.0\n",
      "\n",
      "*** fear ***\n",
      "recall: 0.8571, precision: 0.5243, f1: 0.6506\n",
      "\n",
      "*** anger ***\n",
      "recall: 0.125, precision: 0.0741, f1: 0.093\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "from sklearn.metrics import jaccard_score\n",
    "from datetime import datetime\n",
    "!pip install emoji\n",
    "\n",
    "def evaluate_preprocessing_config(config, val_texts, val_labels):\n",
    "    \"\"\"Quickly evaluate a preprocessing configuration without fine-tuning\"\"\"\n",
    "    try:\n",
    "        # Preprocess validation texts\n",
    "        processed_texts = [pre_process(text, config) for text in tqdm(val_texts, desc=\"Preprocessing\")]\n",
    "        \n",
    "        # Get predictions using base model (without fine-tuning)\n",
    "        predictions = get_predictions_batch(processed_texts, threshold=0.5)\n",
    "        \n",
    "        # Calculate score\n",
    "        score = jaccard_score(val_labels, predictions, average='samples')\n",
    "        \n",
    "        return score\n",
    "    except Exception as e:\n",
    "        print(f\"Error with config {config}: {str(e)}\")\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def grid_search_preprocessing():\n",
    "    # Define all possible configurations\n",
    "    config_options = {\n",
    "        'sep_pn': [False],\n",
    "        'rm_pn': [False],\n",
    "        'apply_lemmatization': [False],\n",
    "        'apply_stemming': [False],\n",
    "        'add_bigrams': [False],\n",
    "        'add_trigrams': [True, False],\n",
    "        'rm_sw': [False],\n",
    "        'handle_emojis': [True],\n",
    "        'expand_contractions': [True],\n",
    "        'normalize_elongated': [True],\n",
    "        'add_pos_tags': [True],\n",
    "        'add_dep_tags': [True],\n",
    "        'use_tfidf': [True],\n",
    "        'tfidf_max_features': [200],\n",
    "        'tfidf_ngram_range': [1, 2],\n",
    "        'tfidf_min_df': [2],\n",
    "        'tfidf_max_df': [0.9]\n",
    "    }\n",
    "\n",
    "    # Generate all possible combinations\n",
    "    keys = config_options.keys()\n",
    "    combinations = [dict(zip(keys, v)) for v in itertools.product(*config_options.values())]\n",
    "    \n",
    "    # Remove invalid combinations\n",
    "    valid_combinations = [config for config in combinations \n",
    "                        if not (config['sep_pn'] and config['rm_pn'])]\n",
    "    \n",
    "    best_scores = {\n",
    "        'jaccard': 0,\n",
    "        'micro_f1': 0,\n",
    "        'macro_f1': 0,\n",
    "        'emotion_f1s': {emotion: 0 for emotion in emotions}\n",
    "    }\n",
    "    best_configs = {\n",
    "        'jaccard': None,\n",
    "        'micro_f1': None,\n",
    "        'macro_f1': None,\n",
    "        'emotion_f1s': {emotion: None for emotion in emotions}\n",
    "    }\n",
    "    results = []\n",
    "    \n",
    "    print(f\"Testing {len(valid_combinations)} configurations...\")\n",
    "    \n",
    "    # Ensure labels are correct format\n",
    "    train_labels = train[emotions].values\n",
    "    val_labels = val[emotions].values\n",
    "    test_labels = test[emotions].values\n",
    "    \n",
    "    # Verify label dimensions\n",
    "    assert train_labels.shape[1] == 5, f\"Train labels shape incorrect: {train_labels.shape}\"\n",
    "    assert val_labels.shape[1] == 5, f\"Validation labels shape incorrect: {val_labels.shape}\"\n",
    "    assert test_labels.shape[1] == 5, f\"Test labels shape incorrect: {test_labels.shape}\"\n",
    "    \n",
    "    for config in tqdm(valid_combinations, desc=\"Grid Search\"):\n",
    "        try:\n",
    "            # Process texts\n",
    "            train_processed = [pre_process(str(text), config) for text in train['text']]\n",
    "            val_processed = [pre_process(str(text), config) for text in val['text']]\n",
    "            test_processed = [pre_process(str(text), config) for text in test['text']]\n",
    "\n",
    "            # Convert tokens to strings for TF-IDF\n",
    "            train_texts = [\" \".join(tokens) for tokens in train_processed]\n",
    "            val_texts = [\" \".join(tokens) for tokens in val_processed]\n",
    "            test_texts = [\" \".join(tokens) for tokens in test_processed]\n",
    "            \n",
    "            # Create TF-IDF features\n",
    "            train_tfidf, val_tfidf, test_tfidf, _ = create_tfidf_features(\n",
    "                train_texts, val_texts, test_texts, config\n",
    "            )\n",
    "            \n",
    "            # Get BERT predictions\n",
    "            val_predictions = get_predictions_batch(val_processed, threshold=0.5)\n",
    "            \n",
    "            # Ensure predictions have 5 labels\n",
    "            if val_predictions.shape[1] != 5:\n",
    "                print(f\"Warning: Predictions shape incorrect: {val_predictions.shape}\")\n",
    "                continue\n",
    "            \n",
    "            # Combine predictions with TF-IDF\n",
    "            val_combined = hstack([val_predictions, val_tfidf]).toarray()\n",
    "            \n",
    "            # Convert to binary predictions\n",
    "            val_final_predictions = (val_combined > 0.5).astype(int)\n",
    "            \n",
    "            # Ensure final predictions have 5 labels\n",
    "            if val_final_predictions.shape[1] != 5:\n",
    "                val_final_predictions = val_final_predictions[:, :5]\n",
    "            \n",
    "            # Calculate all metrics\n",
    "            jaccard = jaccard_score(val[emotions].values, val_final_predictions, average='samples')\n",
    "            micro_f1 = f1_score(val[emotions].values, val_final_predictions, average='micro')\n",
    "            macro_f1 = f1_score(val[emotions].values, val_final_predictions, average='macro')\n",
    "            \n",
    "            # Calculate per-emotion F1 scores\n",
    "            emotion_f1s = {}\n",
    "            for i, emotion in enumerate(emotions):\n",
    "                emotion_f1s[emotion] = f1_score(\n",
    "                    val[emotions].values[:, i], \n",
    "                    val_final_predictions[:, i]\n",
    "                )\n",
    "            \n",
    "            # Store all results\n",
    "            result = {\n",
    "                'config': config,\n",
    "                'jaccard': jaccard,\n",
    "                'micro_f1': micro_f1,\n",
    "                'macro_f1': macro_f1,\n",
    "                'emotion_f1s': emotion_f1s\n",
    "            }\n",
    "            results.append(result)\n",
    "            \n",
    "            # Update best scores - focusing on macro F1\n",
    "            if macro_f1 > best_scores['macro_f1']:\n",
    "                best_scores['macro_f1'] = macro_f1\n",
    "                best_configs['macro_f1'] = config\n",
    "                print(f\"\\nNew best Macro F1: {macro_f1:.4f}\")\n",
    "                print(\"Configuration:\")\n",
    "                for key, value in config.items():\n",
    "                    print(f\"  {key}: {value}\")\n",
    "                print(\"\\nDetailed Metrics:\")\n",
    "                evaluate(val[emotions].values, val_final_predictions)\n",
    "                evaluate_per_class(val[emotions].values, val_final_predictions)\n",
    "            \n",
    "            # Still track other metrics but don't print them\n",
    "            if jaccard > best_scores['jaccard']:\n",
    "                best_scores['jaccard'] = jaccard\n",
    "                best_configs['jaccard'] = config\n",
    "            \n",
    "            if micro_f1 > best_scores['micro_f1']:\n",
    "                best_scores['micro_f1'] = micro_f1\n",
    "                best_configs['micro_f1'] = config\n",
    "            \n",
    "            for emotion in emotions:\n",
    "                if emotion_f1s[emotion] > best_scores['emotion_f1s'][emotion]:\n",
    "                    best_scores['emotion_f1s'][emotion] = emotion_f1s[emotion]\n",
    "                    best_configs['emotion_f1s'][emotion] = config\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error with config {config}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    print(\"\\nFinal Best Configuration (Macro F1):\")\n",
    "    print(f\"Best Macro F1: {best_scores['macro_f1']:.4f}\")\n",
    "    print(\"Configuration:\")\n",
    "    for key, value in best_configs['macro_f1'].items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    return best_configs, best_scores, results\n",
    "# Run grid search\n",
    "print(\"Starting preprocessing grid search...\")\n",
    "best_configs, best_scores, all_results = grid_search_preprocessing()\n",
    "\n",
    "print(\"\\nBest Configurations Found:\")\n",
    "print(f\"Best Jaccard Score: {best_scores['jaccard']:.4f}\")\n",
    "print(\"Configuration:\")\n",
    "for key, value in best_configs['jaccard'].items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\nBest Micro F1: {best_scores['micro_f1']:.4f}\")\n",
    "print(\"Configuration:\")\n",
    "for key, value in best_configs['micro_f1'].items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\nBest Macro F1: {best_scores['macro_f1']:.4f}\")\n",
    "print(\"Configuration:\")\n",
    "for key, value in best_configs['macro_f1'].items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nBest Per-Emotion F1 Scores:\")\n",
    "for emotion in emotions:\n",
    "    print(f\"{emotion}: {best_scores['emotion_f1s'][emotion]:.4f}\")\n",
    "    print(f\"Config: {best_configs['emotion_f1s'][emotion]}\")\n",
    "\n",
    "# Save results to CSV for later analysis\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d_%H_%M_%S')\n",
    "results_df.to_csv(f'../results/preprocessing_grid_search_{timestamp}.csv', index=False)\n",
    "print(f\"\\nDetailed results saved to: ../results/preprocessing_grid_search_{timestamp}.csv\")\n",
    "\n",
    "# ... existing code ...\n",
    "\n",
    "# After grid search is complete, use the macro F1 configuration\n",
    "config = best_configs['macro_f1']  # Use macro F1 config instead of Jaccard\n",
    "print(\"\\nGenerating final predictions with best Macro F1 configuration...\")\n",
    "\n",
    "# Preprocess texts with macro F1 config\n",
    "print(\"Preprocessing texts...\")\n",
    "val_text = [pre_process(text, config) for text in tqdm(val['text'], desc=\"Preprocessing validation\")]\n",
    "test_text = [pre_process(text, config) for text in tqdm(test['text'], desc=\"Preprocessing test\")]\n",
    "\n",
    "# Get predictions\n",
    "print(\"Getting validation predictions...\")\n",
    "val_predictions = get_predictions_batch(val_text, threshold=0.5)\n",
    "\n",
    "print(\"Getting test predictions...\")\n",
    "test_predictions = get_predictions_batch(test_text, threshold=0.5)\n",
    "\n",
    "# Evaluate predictions\n",
    "print(\"\\nFinal Evaluation with Best Macro F1 Configuration:\")\n",
    "val_true = val[emotions].values\n",
    "print(\"Overall Metrics:\")\n",
    "evaluate(val_true, val_predictions)\n",
    "print(\"\\nPer-class Metrics:\")\n",
    "evaluate_per_class(val_true, val_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions to ../results/val_predictions_best_macro_f1_2025-01-30_12_33_01.csv\n",
      "Saved predictions to ../results/test_predictions_best_macro_f1_2025-01-30_12_33_01.csv\n",
      "\n",
      "Now fine-tuning model with best Macro F1 configuration...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing train:   0%|          | 0/2768 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing train: 100%|██████████| 2768/2768 [00:38<00:00, 71.61it/s]\n",
      "Preprocessing val: 100%|██████████| 116/116 [00:01<00:00, 73.31it/s]\n",
      "Preprocessing test: 100%|██████████| 2767/2767 [00:21<00:00, 126.54it/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bhadresh-savani/bert-base-uncased-emotion and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([6]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([6, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model configuration:\n",
      "Classifier weight shape: torch.Size([5, 768])\n",
      "Classifier bias shape: torch.Size([5])\n",
      "Number of labels: 5\n",
      "Emotion labels: ['joy', 'sadness', 'surprise', 'fear', 'anger']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  43%|████▎     | 74/173 [41:27<1:22:26, 49.97s/it]"
     ]
    }
   ],
   "source": [
    "# Save final predictions with macro F1 config\n",
    "save_predictions(\n",
    "    val_predictions,\n",
    "    val['id'],\n",
    "    f'../results/val_predictions_best_macro_f1_{timestamp}.csv'\n",
    ")\n",
    "save_predictions(\n",
    "    test_predictions,\n",
    "    test['id'],\n",
    "    f'../results/test_predictions_best_macro_f1_{timestamp}.csv'\n",
    ")\n",
    "\n",
    "# For fine-tuning, use the macro F1 configuration\n",
    "print(\"\\nNow fine-tuning model with best Macro F1 configuration...\")\n",
    "# Preprocess all data with macro F1 config\n",
    "train_processed = [pre_process(text, config) for text in tqdm(train['text'], desc=\"Preprocessing train\")]\n",
    "val_processed = [pre_process(text, config) for text in tqdm(val['text'], desc=\"Preprocessing val\")]\n",
    "test_processed = [pre_process(text, config) for text in tqdm(test['text'], desc=\"Preprocessing test\")]\n",
    "\n",
    "# Fine-tune model with macro F1 config\n",
    "fine_tuned_model = fine_tune_model(\n",
    "    model,\n",
    "    train_processed,\n",
    "    train[emotions].values,\n",
    "    val_processed,\n",
    "    val[emotions].values,\n",
    "    {\n",
    "        'learning_rate': 2e-5,\n",
    "        'num_epochs': 3,\n",
    "        'batch_size': 16\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search for the best threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the best configuration based on the Macro F1 score from above\n",
    "# Now do grid search for the best threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning the model with the best threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the best threshold found above, fine-tune the model using BERT emotion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 5 Configurations:\n",
    "\n",
    "1. Jaccard Score: 0.3513\n",
    "Configuration:\n",
    "  sep_pn: False\n",
    "  rm_pn: False\n",
    "  apply_lemmatization: False\n",
    "  apply_stemming: False\n",
    "  add_bigrams: False\n",
    "  rm_sw: False\n",
    "\n",
    "2. Jaccard Score: 0.3455\n",
    "Configuration:\n",
    "  sep_pn: False\n",
    "  rm_pn: False\n",
    "  apply_lemmatization: True\n",
    "  apply_stemming: False\n",
    "  add_bigrams: False\n",
    "  rm_sw: False\n",
    "\n",
    "3. Jaccard Score: 0.3441\n",
    "...\n",
    "  apply_lemmatization: False\n",
    "  apply_stemming: False\n",
    "  add_bigrams: False\n",
    "  rm_sw: False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial Results Without Preprocessing:\n",
    "\n",
    "Evaluating validation predictions...\n",
    "Overall Metrics:\n",
    "Multilabel accuracy (Jaccard score): 0.347\n",
    "MICRO recall: 0.3523, precision: 0.6139, f1: 0.4477\n",
    "MACRO recall: 0.4, precision: 0.7202, f1: 0.4234\n",
    "\n",
    "Per-class Metrics:\n",
    "*** joy ***\n",
    "recall: 0.5161, precision: 0.6957, f1: 0.5926\n",
    "\n",
    "*** sadness ***\n",
    "recall: 0.4, precision: 0.875, f1: 0.549\n",
    "\n",
    "*** surprise ***\n",
    "recall: 0.0323, precision: 1.0, f1: 0.0625\n",
    "\n",
    "*** fear ***\n",
    "recall: 0.3016, precision: 0.6552, f1: 0.413\n",
    "\n",
    "*** anger ***\n",
    "recall: 0.75, precision: 0.375, f1: 0.5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

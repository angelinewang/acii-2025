{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "import spacy\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Config\n",
    "config = {\n",
    "    'sep_pn': True, \n",
    "    'rm_pn': False, \n",
    "    'apply_lemmatization': True, \n",
    "    'apply_stemming': True, \n",
    "    'add_bigrams': True, \n",
    "    'rm_sw': False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Add preprocessing functions\n",
    "def pre_process(text, config):\n",
    "    def separate_punctuation(text):\n",
    "        text = re.sub(r\"(\\w)([.,;:!?'\\\"”\\)])\", r\"\\1 \\2\", text)\n",
    "        text = re.sub(r\"([.,;:!?'\\\"“\\(\\)])(\\w)\", r\"\\1 \\2\", text)\n",
    "        return text\n",
    "\n",
    "    def remove_punctuation(text):\n",
    "        text = re.sub(r\"[.,;:!?'\\\"“”\\(\\)]\", \"\", text)\n",
    "        return text\n",
    "\n",
    "    def tokenize_text(text):\n",
    "        encoded_input = tokenizer(text, return_tensors='pt', add_special_tokens=True)\n",
    "        tokens = tokenizer.convert_ids_to_tokens(encoded_input['input_ids'][0])\n",
    "        return tokens\n",
    "\n",
    "    def apply_stemming(tokens):\n",
    "        stemmer = PorterStemmer()\n",
    "        return [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    def apply_lemmatization(tokens):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    def generate_ngrams_from_tokens(tokens, n):\n",
    "        return [\" \".join(gram) for gram in ngrams(tokens, n)]\n",
    "\n",
    "    # Apply config options\n",
    "    if config['sep_pn'] and not config['rm_pn']:\n",
    "        text = separate_punctuation(text)\n",
    "    if config['rm_pn'] and not config['sep_pn']:\n",
    "        text = remove_punctuation(text)\n",
    "\n",
    "    tokens = tokenize_text(text)\n",
    "    if config['apply_stemming']:\n",
    "        tokens = apply_stemming(tokens)\n",
    "    if config['apply_lemmatization']:\n",
    "        tokens = apply_lemmatization(tokens)\n",
    "    if config['add_bigrams']:\n",
    "        tokens += generate_ngrams_from_tokens(tokens, 2)\n",
    "    if config['rm_sw']:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model and tokenizer\n",
    "model_name = \"bhadresh-savani/bert-base-uncased-emotion\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/angwang/miniforge3/lib/python3.12/site-packages/transformers/pipelines/text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create the emotion classifier pipeline\n",
    "classifier = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_all_scores=True,\n",
    "    device=0 if torch.cuda.is_available() else -1  # Use GPU if available\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_batch(texts, batch_size=32):\n",
    "    \"\"\"\n",
    "    Get predictions for a list of texts in batches\n",
    "    \"\"\"\n",
    "    all_predictions = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Predicting\"):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        predictions = classifier(batch_texts)\n",
    "        all_predictions.extend(predictions)\n",
    "    \n",
    "    return all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_predictions(predictions, emotion_mapping):\n",
    "    \"\"\"\n",
    "    Convert raw predictions to the required format\n",
    "    \"\"\"\n",
    "    formatted_preds = []\n",
    "    for pred in predictions:\n",
    "        # Create a dictionary of emotion scores\n",
    "        scores = {p['label']: p['score'] for p in pred}\n",
    "        # Map to our required emotions and format\n",
    "        row = [scores.get(emotion_mapping.get(e, e), 0) for e in emotions]\n",
    "        formatted_preds.append(row)\n",
    "    \n",
    "    return np.array(formatted_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train = pd.read_csv('../public_data_test/track_a/train/eng.csv')\n",
    "val = pd.read_csv('../public_data_test/track_a/dev/eng.csv')\n",
    "test = pd.read_csv('../public_data_test/track_a/test/eng.csv')\n",
    "\n",
    "# Define emotions and mapping (model's labels to our labels)\n",
    "emotions = ['joy', 'sadness', 'surprise', 'fear', 'anger']\n",
    "emotion_mapping = {\n",
    "    'joy': 'joy',\n",
    "    'sadness': 'sadness',\n",
    "    'surprise': 'surprise',\n",
    "    'fear': 'fear',\n",
    "    'anger': 'anger',\n",
    "    'love': None  # We'll ignore this emotion as it's not in our target set\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction\n",
    "vectorizer = CountVectorizer()\n",
    "train_text = [pre_process(text, config) for text in train[\"text\"]]\n",
    "val_text = [pre_process(text, config) for text in val[\"text\"]]\n",
    "test_text = [pre_process(text, config) for text in test[\"text\"]]\n",
    "\n",
    "X_train = vectorizer.fit_transform(train_text).toarray()\n",
    "X_val = vectorizer.transform(val_text).toarray()\n",
    "X_test = vectorizer.transform(test_text).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to numpy arrays\n",
    "y_train = train[emotions].values\n",
    "y_val = val[emotions].values\n",
    "y_test = test[emotions].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting training BERT embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 87/87 [07:33<00:00,  5.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting validation BERT embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 4/4 [00:14<00:00,  3.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting test BERT embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 87/87 [39:48<00:00, 27.45s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final feature dimensions: (2768, 4079)\n"
     ]
    }
   ],
   "source": [
    "# Get BERT embeddings for training data\n",
    "print(\"Getting training BERT embeddings...\")\n",
    "train_raw_preds = get_predictions_batch(train['text'].tolist())\n",
    "train_bert_features = format_predictions(train_raw_preds, emotion_mapping)\n",
    "\n",
    "print(\"Getting validation BERT embeddings...\")\n",
    "val_raw_preds = get_predictions_batch(val['text'].tolist())\n",
    "val_bert_features = format_predictions(val_raw_preds, emotion_mapping)\n",
    "\n",
    "print(\"Getting test BERT embeddings...\")\n",
    "test_raw_preds = get_predictions_batch(test['text'].tolist())\n",
    "test_bert_features = format_predictions(test_raw_preds, emotion_mapping)\n",
    "\n",
    "# Combine CountVectorizer features with BERT embeddings\n",
    "final_train_features = np.hstack([X_train, train_bert_features])\n",
    "final_val_features = np.hstack([X_val, val_bert_features])\n",
    "final_test_features = np.hstack([X_test, test_bert_features])\n",
    "\n",
    "print(f\"Final feature dimensions: {final_train_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert probabilities to binary predictions (you can adjust the threshold)\n",
    "threshold = 0.45\n",
    "val_binary_preds = (val_bert_features > threshold).astype(int)\n",
    "test_binary_preds = (test_bert_features > threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(final_train_features.shape[1], 128),\n",
    "    nn.BatchNorm1d(128),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.4),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, y_train.shape[1])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "features_tensor = torch.tensor(final_train_features, dtype=torch.float32)\n",
    "labels_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "dataset = TensorDataset(features_tensor, labels_tensor)\n",
    "data_loader = DataLoader(dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weights\n",
    "weights = y_train.sum(axis=0)/y_train.sum()\n",
    "weights = max(weights)/weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.Tensor(weights))\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-4, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop:   0%|          | 1/401 [00:01<06:59,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss: 0.839\n",
      "Saved epoch 0 weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop:  25%|██▌       | 101/401 [01:23<04:29,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100: Loss: 0.886\n",
      "Saved epoch 100 weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop:  50%|█████     | 201/401 [02:57<02:55,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200: Loss: 0.732\n",
      "Saved epoch 200 weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop:  75%|███████▌  | 301/401 [04:25<01:24,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300: Loss: 0.897\n",
      "Saved epoch 300 weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop: 100%|██████████| 401/401 [05:44<00:00,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400: Loss: 0.667\n",
      "Saved epoch 400 weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "losses = []\n",
    "for epoch in tqdm(range(401), desc=\"Training Loop\"):\n",
    "    for features, labels in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch}: Loss: {round(loss.item(),3)}')\n",
    "        torch.save(model.state_dict(), f'./model_checkpoints/net_epoch_{epoch}.pth')\n",
    "        print(f\"Saved epoch {epoch} weights\")\n",
    "        losses.append(round(loss.item(),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions\n",
    "def save_predictions(predictions, ids, filename):\n",
    "    df_predictions = pd.DataFrame(predictions, columns=emotions)\n",
    "    df_predictions['id'] = ids\n",
    "    df_predictions = df_predictions[['id'] + emotions]\n",
    "    df_predictions.to_csv(filename, index=False)\n",
    "    print(f\"Saved predictions to {filename}\")\n",
    "\n",
    "# Save validation and test predictions\n",
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d_%H_%M_%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sx/hkqgssrx0jd_jb2_792hz_hm0000gp/T/ipykernel_2128/3075013302.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f'./model_checkpoints/net_epoch_{best_epoch}.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions to ../results/val_predictions_epoch_400_2025-01-30_14_58_37.csv\n",
      "Saved predictions to ../results/test_predictions_epoch_400_2025-01-30_14_58_37.csv\n"
     ]
    }
   ],
   "source": [
    "# After training loop, load the best model checkpoint\n",
    "best_epoch = 400  # or whichever epoch had the best performance\n",
    "model.load_state_dict(torch.load(f'./model_checkpoints/net_epoch_{best_epoch}.pth'))\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "# Generate predictions using the loaded model\n",
    "with torch.no_grad():  # No need to track gradients during inference\n",
    "    val_preds = model(torch.tensor(final_val_features, dtype=torch.float32))\n",
    "    test_preds = model(torch.tensor(final_test_features, dtype=torch.float32))\n",
    "\n",
    "# Convert to probabilities using sigmoid\n",
    "sigmoid = nn.Sigmoid()\n",
    "val_probs = sigmoid(val_preds).numpy()\n",
    "test_probs = sigmoid(test_preds).numpy()\n",
    "\n",
    "# Convert to binary predictions\n",
    "threshold = 0.45\n",
    "val_binary_preds = (val_probs > threshold).astype(int)\n",
    "test_binary_preds = (test_probs > threshold).astype(int)\n",
    "\n",
    "# Save predictions\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d_%H_%M_%S')\n",
    "save_predictions(\n",
    "    val_binary_preds,\n",
    "    val['id'],\n",
    "    f'../results/val_predictions_epoch_{best_epoch}_{timestamp}.csv'\n",
    ")\n",
    "\n",
    "save_predictions(\n",
    "    test_binary_preds,\n",
    "    test['id'],\n",
    "    f'../results/test_predictions_epoch_{best_epoch}_{timestamp}.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Set Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         joy       0.56      0.71      0.63        31\n",
      "     sadness       0.47      0.77      0.59        35\n",
      "    surprise       0.36      0.77      0.49        31\n",
      "        fear       0.55      1.00      0.71        63\n",
      "       anger       0.35      0.69      0.47        16\n",
      "\n",
      "   micro avg       0.48      0.84      0.61       176\n",
      "   macro avg       0.46      0.79      0.58       176\n",
      "weighted avg       0.49      0.84      0.61       176\n",
      " samples avg       0.51      0.77      0.58       176\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/angwang/miniforge3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Evaluate validation predictions\n",
    "print(\"\\nValidation Set Performance:\")\n",
    "print(classification_report(\n",
    "    val[emotions].values,\n",
    "    val_binary_preds,\n",
    "    target_names=emotions\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
